{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279c8a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLNOTEBOOK.ipynb',\n",
       " 'X_test_clean.joblib',\n",
       " 'cleaned_SIG_YOUSSER.csv',\n",
       " 'cleaned_dataset.csv',\n",
       " 'info.ipynb',\n",
       " 'rf_model.joblib',\n",
       " 'training_dataset_balanced.csv',\n",
       " 'y_test.joblib']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, csv, codecs\n",
    "from typing import Dict, Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "# Notebook and files are in the same folder\n",
    "RAW_DIR = Path(\".\")   # current directory = raw_data\n",
    "\n",
    "# Quick sanity check: list files here\n",
    "sorted([p.name for p in RAW_DIR.iterdir() if p.is_file()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6677ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sample_bytes(path: Path, n_bytes: int = 512_000) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read(n_bytes)\n",
    "\n",
    "def sniff_delimiter_and_header(path: Path) -> Tuple[str, bool]:\n",
    "    sample = read_sample_bytes(path)\n",
    "    text = None\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin-1\"):\n",
    "        try:\n",
    "            text = sample.decode(enc)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    if text is None:\n",
    "        text = sample.decode(\"latin-1\", errors=\"replace\")\n",
    "\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(text, delimiters=[\",\", \";\", \"\\t\", \"|\", \":\"])\n",
    "        has_header = csv.Sniffer().has_header(text)\n",
    "        return dialect.delimiter, has_header\n",
    "    except csv.Error:\n",
    "        lines = [ln for ln in text.splitlines() if ln.strip()]\n",
    "        first = lines[0] if lines else \"\"\n",
    "        candidates = [\",\", \";\", \"\\t\", \"|\", \":\"]\n",
    "        counts = {d: len(first.split(d)) for d in candidates}\n",
    "        best = max(counts, key=counts.get)\n",
    "        tokens = [t.strip().strip('\"') for t in first.split(best)]\n",
    "        has_header = any(not re.fullmatch(r\"-?\\d+(\\.\\d+)?\", t) for t in tokens if t)\n",
    "        return best, has_header\n",
    "\n",
    "def count_columns(path: Path, delimiter: str) -> int:\n",
    "    # first non-empty line (header or first row)\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin-1\"):\n",
    "        try:\n",
    "            with codecs.open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        return len(next(csv.reader([line], delimiter=delimiter)))\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    with codecs.open(path, \"r\", encoding=\"latin-1\", errors=\"replace\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                return len(next(csv.reader([line], delimiter=delimiter)))\n",
    "    return 0\n",
    "\n",
    "def fast_count_lines(path: Path) -> int:\n",
    "    nl = 0\n",
    "    with open(path, \"rb\", buffering=1024 * 1024) as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            nl += chunk.count(b\"\\n\")\n",
    "    if nl == 0 and path.stat().st_size > 0:\n",
    "        return 1\n",
    "    return nl\n",
    "\n",
    "def fmt_int(n: int) -> str:\n",
    "    return f\"{n:,}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa667ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DESCRIPTIONS: Dict[str, str] = {\n",
    "    r\"\\bSIG_YOUSSER\\b|\\bSIG\\b\": \"Données démographiques clients\",\n",
    "    r\"\\bCREDIT_YOUSSER\\b|\\bCREDIT\\b\": \"Historique des crédits\",\n",
    "    r\"\\bCHAABI[_\\s]*MOBILE_YOUSSER\\b\": \"Produits mobiles\",\n",
    "    r\"\\bCHAABI[_\\s]*NET_YOUSSER\\b\": \"Produits net banking\",\n",
    "    r\"\\bPRODUIT[_\\s]*PACK_YOUSSER\\b\": \"Packs / offres produits\",\n",
    "    r\"\\bPRODUIT[_\\s]*BANCASSURANCE_YOUSSER\\b\": \"Produits bancassurance\",\n",
    "    r\"\\bCARTE_YOUSSER\\b\": \"Cartes bancaires\",\n",
    "    r\"\\bCompte_YOUSSER\\b|\\bCOMPTE\\b\": \"Comptes clients\",\n",
    "}\n",
    "\n",
    "# Force header presence/absence if needed (filename -> True/False)\n",
    "HEADER_OVERRIDES: Dict[str, bool] = {\n",
    "    # \"Compte_YOUSSER\": True,\n",
    "    # \"SIG_YOUSSER\": True,\n",
    "}\n",
    "\n",
    "def best_description(fname: str) -> str:\n",
    "    for pat, desc in FILE_DESCRIPTIONS.items():\n",
    "        if re.search(pat, fname, flags=re.IGNORECASE):\n",
    "            return desc\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ecca30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Lignes</th>\n",
       "      <th>Colonnes</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_dataset.csv</td>\n",
       "      <td>5530486</td>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_SIG_YOUSSER.csv</td>\n",
       "      <td>6700655</td>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training_dataset_balanced.csv</td>\n",
       "      <td>8800174</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total</td>\n",
       "      <td>21031315</td>\n",
       "      <td></td>\n",
       "      <td>Volume global</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Source    Lignes Colonnes    Description\n",
       "0            cleaned_dataset.csv   5530486       19               \n",
       "1        cleaned_SIG_YOUSSER.csv   6700655       13               \n",
       "2  training_dataset_balanced.csv   8800174       18               \n",
       "3                          Total  21031315           Volume global"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_inventory(raw_dir: Path) -> pd.DataFrame:\n",
    "    rows: List[dict] = []\n",
    "\n",
    "    # include .txt, .csv, and files without extension; exclude notebooks\n",
    "    candidates = [\n",
    "        p for p in raw_dir.iterdir()\n",
    "        if p.is_file() and p.suffix.lower() not in {\".ipynb\"} and p.name != \".ipynb_checkpoints\"\n",
    "           and (p.suffix.lower() in {\".txt\", \".csv\"} or p.suffix == \"\")\n",
    "    ]\n",
    "\n",
    "    for p in sorted(candidates, key=lambda x: x.name.lower()):\n",
    "        delimiter, detected_header = sniff_delimiter_and_header(p)\n",
    "        has_header = HEADER_OVERRIDES.get(p.name, detected_header)\n",
    "\n",
    "        total_lines = fast_count_lines(p)\n",
    "        n_rows = max(total_lines - (1 if has_header else 0), 0)\n",
    "        n_cols = count_columns(p, delimiter)\n",
    "\n",
    "        rows.append({\n",
    "            \"Source\": p.name,\n",
    "            \"Lignes\": n_rows,\n",
    "            \"Colonnes\": n_cols,\n",
    "            \"Description\": best_description(p.name),\n",
    "        })\n",
    "\n",
    "    # Add total\n",
    "    df = pd.DataFrame(rows, columns=[\"Source\", \"Lignes\", \"Colonnes\", \"Description\"])\n",
    "    total = pd.DataFrame([{\n",
    "        \"Source\": \"Total\",\n",
    "        \"Lignes\": int(df[\"Lignes\"].sum()) if not df.empty else 0,\n",
    "        \"Colonnes\": \"\",\n",
    "        \"Description\": \"Volume global\",\n",
    "    }])\n",
    "    return pd.concat([df, total], ignore_index=True)\n",
    "\n",
    "df_info = build_inventory(RAW_DIR)\n",
    "df_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c5235a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source | Lignes | Colonnes | Description\n",
      "------------------------------------------------------------------------------------------\n",
      "cleaned_dataset.csv | 5,530,486 | 19 | \n",
      "cleaned_SIG_YOUSSER.csv | 6,700,655 | 13 | \n",
      "training_dataset_balanced.csv | 8,800,174 | 18 | \n",
      "------------------------------------------------------------------------------------------\n",
      "Total | 21,031,315 |  | Volume global\n"
     ]
    }
   ],
   "source": [
    "def print_summary_for_report(df: pd.DataFrame):\n",
    "    print(\"Source | Lignes | Colonnes | Description\")\n",
    "    print(\"-\" * 90)\n",
    "    for _, r in df.iloc[:-1].iterrows():\n",
    "        print(f\"{r['Source']} | {fmt_int(int(r['Lignes']))} | {r['Colonnes']} | {r['Description']}\")\n",
    "    r = df.iloc[-1]\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{r['Source']} | {fmt_int(int(r['Lignes']))} |  | {r['Description']}\")\n",
    "\n",
    "print_summary_for_report(df_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18f01fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_26736\\2631374897.py:4: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"cleaned_dataset.csv\")  # ou CSV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients uniques : 5,530,486\n",
      "Lignes totales  : 5,530,486\n",
      "Variables       : 19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chargement du dataset final\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")  # ou CSV\n",
    "\n",
    "# Nombre d'observations uniques\n",
    "nb_clients = df[\"ID_CLIENT\"].nunique()\n",
    "\n",
    "# Nombre total de lignes\n",
    "nb_rows = len(df)\n",
    "\n",
    "# Nombre de colonnes\n",
    "nb_cols = df.shape[1]\n",
    "\n",
    "print(f\"Clients uniques : {nb_clients:,}\")\n",
    "print(f\"Lignes totales  : {nb_rows:,}\")\n",
    "print(f\"Variables       : {nb_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ccdc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2693.06990814209 MB\n"
     ]
    }
   ],
   "source": [
    "print(df.memory_usage(deep=True).sum() / (1024**2), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5748fca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démographiques 4 ['SEXE', 'DATE_NAISSANCE', 'NOMBRE_ENFANT', 'MARITAL_STATUS']\n",
      "Géographiques 4 ['VILLE', 'COUNTRY', 'RESIDENCE', 'CODE_VILLE']\n",
      "Socio-économiques 2 ['PROFESSION', 'FLAG_PROPRIETAIRE_LOGEMENT']\n",
      "Produits 6 ['has_mobile', 'has_net', 'has_pack', 'has_bancassurance', 'products_count', 'digital_intensity']\n",
      "Variables cibles 2 ['credit_obtenu', 'MT_ACCORDE']\n"
     ]
    }
   ],
   "source": [
    "categories = {\n",
    "    \"Démographiques\": [\"SEXE\", \"DATE_NAISSANCE\", \"NOMBRE_ENFANT\", \"MARITAL_STATUS\"],\n",
    "    \"Géographiques\": [\"VILLE\", \"COUNTRY\", \"RESIDENCE\", \"CODE_VILLE\"],\n",
    "    \"Socio-économiques\": [\"PROFESSION\", \"FLAG_PROPRIETAIRE_LOGEMENT\"],\n",
    "    \"Produits\": [\"has_mobile\", \"has_net\", \"has_pack\", \"has_bancassurance\", \"products_count\", \"digital_intensity\"],\n",
    "    \"Variables cibles\": [\"credit_obtenu\", \"MT_ACCORDE\"]\n",
    "}\n",
    "\n",
    "for cat, vars_list in categories.items():\n",
    "    print(cat, len(vars_list), vars_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
